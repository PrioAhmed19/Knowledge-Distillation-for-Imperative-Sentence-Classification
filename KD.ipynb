{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VSkB-hjGFOls"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from torch.optim import AdamW # Changed import to use torch.optim.AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT9cwI32FihD",
        "outputId": "077de853-da00-4171-dc97-3ca2bf06e7e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O Final-dataset.csv \"https://www.dropbox.com/scl/fi/v7olloa8to9ixjvp3my2l/Final-dataset.csv?rlkey=zk1aasrpcaop79cfgogufs76q&st=jf24beip&dl=0\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBnqpP2HCcwt",
        "outputId": "c1ec1d18-3689-4938-cea2-7e254433027f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-16 18:24:18--  https://www.dropbox.com/scl/fi/v7olloa8to9ixjvp3my2l/Final-dataset.csv?rlkey=zk1aasrpcaop79cfgogufs76q&st=jf24beip&dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.2.18, 2620:100:6021:18::a27d:4112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.2.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uccc398edb3f9e9ef4edb5e12e23.dl.dropboxusercontent.com/cd/0/inline/C5H3FgfdtXfEg1YDoPG3VI05rruK4v_Y24xPJdaifaFOzU3ECBuYf2uB36vHaPnfTler99nOSijJpM_u6JvousADGATqwy9L2Bplbsj0ejDBfY6PfZ3tmkuUeP41DQThwLncDkRZ9fTfuXq-v-bN5c-p/file# [following]\n",
            "--2026-01-16 18:24:19--  https://uccc398edb3f9e9ef4edb5e12e23.dl.dropboxusercontent.com/cd/0/inline/C5H3FgfdtXfEg1YDoPG3VI05rruK4v_Y24xPJdaifaFOzU3ECBuYf2uB36vHaPnfTler99nOSijJpM_u6JvousADGATqwy9L2Bplbsj0ejDBfY6PfZ3tmkuUeP41DQThwLncDkRZ9fTfuXq-v-bN5c-p/file\n",
            "Resolving uccc398edb3f9e9ef4edb5e12e23.dl.dropboxusercontent.com (uccc398edb3f9e9ef4edb5e12e23.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to uccc398edb3f9e9ef4edb5e12e23.dl.dropboxusercontent.com (uccc398edb3f9e9ef4edb5e12e23.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 288603 (282K) [text/plain]\n",
            "Saving to: ‘Final-dataset.csv’\n",
            "\n",
            "Final-dataset.csv   100%[===================>] 281.84K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2026-01-16 18:24:19 (12.7 MB/s) - ‘Final-dataset.csv’ saved [288603/288603]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "a0HqHgdQFWib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Final-dataset.csv')"
      ],
      "metadata": {
        "id": "f1FZLewXFaNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bek4HwVFzWk",
        "outputId": "9df5175a-e836-40fe-9858-7a81d5d3a307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 sentence  label\n",
            "0  please kalke assignment ta submit koro      1\n",
            "1            vai eita ektu check kore dio      1\n",
            "2                        urgent kotha ase      1\n",
            "3             client already wait kortese      1\n",
            "4     oi link ta open korish na virus ase      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_df = df.sample(frac=1, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "ICLKk27uF0-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(shuffled_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5myshmoGCwK",
        "outputId": "f391b010-830c-4a06-e7de-8d6074a09998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     sentence  label\n",
            "0                         Math ta bujhiye de.      1\n",
            "1                 Please amake ektu help kor.      1\n",
            "2    ei report ta ami download korte parbo na      0\n",
            "3  dude joldi koire felo na hole problem hobe      1\n",
            "4                     exam er por ghure ashbo      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = shuffled_df.copy()"
      ],
      "metadata": {
        "id": "Knwr8yp9GEbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data splitting\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Split into train (70%) and temp (30%)\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    df['sentence'].tolist(),\n",
        "    df['label'].tolist(),\n",
        "    test_size=0.3,\n",
        "    stratify=df['label'],\n",
        "    random_state=42,\n",
        "\n",
        ")\n",
        "\n",
        "# Step 2: Split temp into validation (15%) and test (15%)\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts,\n",
        "    temp_labels,\n",
        "    test_size=0.5,\n",
        "    stratify=train_val_labels,\n",
        "    random_state=42,\n",
        ")"
      ],
      "metadata": {
        "id": "gweLUICsGLzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nTrain set: {len(train_texts)} samples\")\n",
        "print(f\"Validation set: {len(val_texts)} samples\")\n",
        "print(f\"Validation set: {len(test_texts)} samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lMJc0_OGnjc",
        "outputId": "b295945c-2045-42b2-faab-02d6a4f5ca9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train set: 6157 samples\n",
            "Validation set: 1320 samples\n",
            "Validation set: 1320 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization, and batching during training\n",
        "\n",
        "\n",
        "class ImperativeDataset(Dataset):\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize the text\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "AdCAW4HOGrF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Teacher Model (Large - better accuracy)\n",
        "teacher_model_name = 'bert-base-uncased'\n",
        "print(f\"\\nTeacher Model: {teacher_model_name}\")\n",
        "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n",
        "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    teacher_model_name,\n",
        "    num_labels=2\n",
        ").to(device)\n",
        "\n",
        "# Student Model (Small - efficient)\n",
        "student_model_name = 'prajjwal1/bert-tiny'\n",
        "print(f\"Student Model: {student_model_name}\")\n",
        "student_tokenizer = AutoTokenizer.from_pretrained(student_model_name)\n",
        "student_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    student_model_name,\n",
        "    num_labels=2\n",
        ").to(device)\n",
        "\n",
        "print(f\"\\nTeacher parameters: {sum(p.numel() for p in teacher_model.parameters()):,}\")\n",
        "print(f\"Student parameters: {sum(p.numel() for p in student_model.parameters()):,}\")\n",
        "print(f\"Compression ratio: {sum(p.numel() for p in teacher_model.parameters()) / sum(p.numel() for p in student_model.parameters()):.2f}x\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbl-dL-hHbyc",
        "outputId": "07ec3040-f2c8-4107-d480-c39a02f88e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Teacher Model: bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student Model: prajjwal1/bert-tiny\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Teacher parameters: 109,483,778\n",
            "Student parameters: 4,386,178\n",
            "Compression ratio: 24.96x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nWhy: The teacher needs to be trained first so it can provide 'soft labels'\")\n",
        "print(\"(probability distributions) that contain richer information than hard labels.\\n\")\n",
        "\n",
        "# Create datasets and dataloaders for teacher\n",
        "teacher_train_dataset = ImperativeDataset(train_texts, train_labels, teacher_tokenizer)\n",
        "teacher_val_dataset = ImperativeDataset(val_texts, val_labels, teacher_tokenizer)\n",
        "teacher_test_dataset = ImperativeDataset(test_texts, test_labels, teacher_tokenizer)\n",
        "\n",
        "teacher_train_loader = DataLoader(teacher_train_dataset, batch_size=16, shuffle=True)\n",
        "teacher_val_loader = DataLoader(teacher_val_dataset, batch_size=16, shuffle=False)\n",
        "teacher_test_loader = DataLoader(teacher_test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Training configuration for teacher\n",
        "teacher_epochs = 5\n",
        "teacher_optimizer = AdamW(teacher_model.parameters(), lr=2e-5)\n",
        "teacher_scheduler = get_linear_schedule_with_warmup(\n",
        "    teacher_optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=len(teacher_train_loader) * teacher_epochs\n",
        ")\n",
        "\n",
        "def train_teacher(model, train_loader, optimizer, scheduler, device):\n",
        "    \"\"\"Train the teacher model with standard cross-entropy loss.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=\"Training Teacher\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate_model(model, val_loader, device):\n",
        "    \"\"\"Evaluate model accuracy.\"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    return accuracy, predictions, true_labels\n",
        "\n",
        "# Train teacher model\n",
        "print(\"\\nTraining teacher model...\")\n",
        "for epoch in range(teacher_epochs):\n",
        "    train_loss = train_teacher(teacher_model, teacher_train_loader, teacher_optimizer, teacher_scheduler, device)\n",
        "    val_accuracy, _, _ = evaluate_model(teacher_model, teacher_val_loader, device)\n",
        "    print(f\"Epoch {epoch+1}/{teacher_epochs} - Loss: {train_loss:.4f} - Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nTeacher model training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJAhxwLgJwmT",
        "outputId": "519f4580-dd05-4daf-cd22-98aa609cdb64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Why: The teacher needs to be trained first so it can provide 'soft labels'\n",
            "(probability distributions) that contain richer information than hard labels.\n",
            "\n",
            "\n",
            "Training teacher model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Teacher: 100%|██████████| 385/385 [02:18<00:00,  2.79it/s]\n",
            "Evaluating: 100%|██████████| 83/83 [00:09<00:00,  8.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Loss: 0.2009 - Val Accuracy: 0.9682\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Teacher: 100%|██████████| 385/385 [02:14<00:00,  2.87it/s]\n",
            "Evaluating: 100%|██████████| 83/83 [00:09<00:00,  8.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5 - Loss: 0.0557 - Val Accuracy: 0.9727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Teacher: 100%|██████████| 385/385 [02:13<00:00,  2.88it/s]\n",
            "Evaluating: 100%|██████████| 83/83 [00:09<00:00,  8.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5 - Loss: 0.0311 - Val Accuracy: 0.9735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Teacher: 100%|██████████| 385/385 [02:11<00:00,  2.93it/s]\n",
            "Evaluating: 100%|██████████| 83/83 [00:09<00:00,  8.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5 - Loss: 0.0155 - Val Accuracy: 0.9712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Teacher: 100%|██████████| 385/385 [02:11<00:00,  2.92it/s]\n",
            "Evaluating: 100%|██████████| 83/83 [00:09<00:00,  8.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5 - Loss: 0.0091 - Val Accuracy: 0.9705\n",
            "\n",
            "Teacher model training completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STEP 5: DEFINING DISTILLATION LOSS\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nWhy: Knowledge distillation uses a special loss that combines:\")\n",
        "print(\"1. Soft targets from teacher (captures uncertainty and relationships)\")\n",
        "print(\"2. Hard labels from ground truth (ensures correctness)\")\n",
        "print(\"3. Temperature parameter (controls softness of probability distribution)\\n\")\n",
        "\n",
        "class DistillationLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Knowledge Distillation Loss combines:\n",
        "    - KL divergence between student and teacher soft predictions\n",
        "    - Cross-entropy loss with true labels\n",
        "\n",
        "    Temperature (T): Higher T makes probabilities softer, revealing more\n",
        "    information about what the teacher learned.\n",
        "\n",
        "    Alpha: Balance between learning from teacher vs. learning from labels\n",
        "    \"\"\"\n",
        "    def __init__(self, temperature=3.0, alpha=0.7):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, labels):\n",
        "        # Soft targets: Apply temperature to soften probability distributions\n",
        "        # Why temperature? It makes the teacher's \"confidence\" more informative\n",
        "        soft_student = F.log_softmax(student_logits / self.temperature, dim=1)\n",
        "        soft_teacher = F.softmax(teacher_logits / self.temperature, dim=1)\n",
        "\n",
        "        # Distillation loss: How well student mimics teacher's soft predictions\n",
        "        distillation_loss = self.kl_div(soft_student, soft_teacher) * (self.temperature ** 2)\n",
        "\n",
        "        # Student loss: Standard cross-entropy with true labels\n",
        "        student_loss = self.ce_loss(student_logits, labels)\n",
        "\n",
        "        # Combined loss: Weighted average\n",
        "        # alpha controls balance: higher alpha = learn more from teacher\n",
        "        total_loss = self.alpha * distillation_loss + (1 - self.alpha) * student_loss\n",
        "\n",
        "        return total_loss, distillation_loss, student_loss\n",
        "\n",
        "print(\"Distillation Loss Configuration:\")\n",
        "print(f\"  Temperature: 3.0 (softens probabilities for richer knowledge transfer)\")\n",
        "print(f\"  Alpha: 0.7 (70% teacher knowledge, 30% ground truth)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zENgJ-sRJ7Xc",
        "outputId": "5fa6e80f-892a-4a62-8df5-a39ef20ec2bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 5: DEFINING DISTILLATION LOSS\n",
            "================================================================================\n",
            "\n",
            "Why: Knowledge distillation uses a special loss that combines:\n",
            "1. Soft targets from teacher (captures uncertainty and relationships)\n",
            "2. Hard labels from ground truth (ensures correctness)\n",
            "3. Temperature parameter (controls softness of probability distribution)\n",
            "\n",
            "Distillation Loss Configuration:\n",
            "  Temperature: 3.0 (softens probabilities for richer knowledge transfer)\n",
            "  Alpha: 0.7 (70% teacher knowledge, 30% ground truth)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 6: DISTILLING KNOWLEDGE TO STUDENT MODEL\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nWhy: Now we train the student to mimic the teacher's behavior while also\")\n",
        "print(\"learning from the true labels. This allows the small model to perform\")\n",
        "print(\"nearly as well as the large model.\\n\")\n",
        "\n",
        "# Create datasets and dataloaders for student\n",
        "student_train_dataset = ImperativeDataset(train_texts, train_labels, student_tokenizer)\n",
        "student_val_dataset = ImperativeDataset(val_texts, val_labels, student_tokenizer)\n",
        "student_test_dataset = ImperativeDataset(test_texts, test_labels, student_tokenizer)\n",
        "\n",
        "student_train_loader = DataLoader(student_train_dataset, batch_size=16, shuffle=True)\n",
        "student_val_loader = DataLoader(student_val_dataset, batch_size=16, shuffle=False)\n",
        "student_test_loader = DataLoader(student_test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Training configuration for student\n",
        "student_epochs = 5  # More epochs for distillation\n",
        "student_optimizer = AdamW(student_model.parameters(), lr=5e-5)\n",
        "student_scheduler = get_linear_schedule_with_warmup(\n",
        "    student_optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=len(student_train_loader) * student_epochs\n",
        ")\n",
        "\n",
        "distillation_loss_fn = DistillationLoss(temperature=3.0, alpha=0.7)\n",
        "\n",
        "def train_student_with_distillation(student_model, teacher_model, train_loader,\n",
        "                                   optimizer, scheduler, loss_fn, device):\n",
        "    \"\"\"\n",
        "    Train student model using knowledge distillation.\n",
        "    Student learns from both teacher's soft predictions and true labels.\n",
        "    \"\"\"\n",
        "    student_model.train()\n",
        "    teacher_model.eval()  # Teacher in eval mode (no training)\n",
        "\n",
        "    total_loss = 0\n",
        "    total_distill_loss = 0\n",
        "    total_student_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=\"Distilling Knowledge\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get student inputs\n",
        "        student_input_ids = batch['input_ids'].to(device)\n",
        "        student_attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "\n",
        "        # Student forward pass\n",
        "        student_outputs = student_model(\n",
        "            input_ids=student_input_ids,\n",
        "            attention_mask=student_attention_mask\n",
        "        )\n",
        "        student_logits = student_outputs.logits\n",
        "\n",
        "        # Teacher forward pass (no gradient)\n",
        "        with torch.no_grad():\n",
        "            teacher_outputs = teacher_model(\n",
        "                input_ids=student_input_ids,  # Using same tokens (simplified)\n",
        "                attention_mask=student_attention_mask\n",
        "            )\n",
        "            teacher_logits = teacher_outputs.logits\n",
        "\n",
        "        # Calculate distillation loss\n",
        "        loss, distill_loss, ce_loss = loss_fn(student_logits, teacher_logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_distill_loss += distill_loss.item()\n",
        "        total_student_loss += ce_loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    avg_distill = total_distill_loss / len(train_loader)\n",
        "    avg_student = total_student_loss / len(train_loader)\n",
        "\n",
        "    return avg_loss, avg_distill, avg_student\n",
        "\n",
        "# Train student with distillation\n",
        "print(\"Training student model with knowledge distillation...\")\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(student_epochs):\n",
        "    # Train with distillation\n",
        "    loss, distill_loss, ce_loss = train_student_with_distillation(\n",
        "        student_model, teacher_model, student_train_loader,\n",
        "        student_optimizer, student_scheduler, distillation_loss_fn, device\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    val_accuracy, _, _ = evaluate_model(student_model, student_val_loader, device)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}/{student_epochs}\")\n",
        "    print(f\"  Total Loss: {loss:.4f}\")\n",
        "    print(f\"  Distillation Loss: {distill_loss:.4f}\")\n",
        "    print(f\"  Student CE Loss: {ce_loss:.4f}\")\n",
        "    print(f\"  Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    if val_accuracy > best_accuracy:\n",
        "        best_accuracy = val_accuracy\n",
        "        torch.save(student_model.state_dict(), 'best_student_model.pt')\n",
        "        print(f\"  ✓ New best model saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnEr6AsIKDPq",
        "outputId": "a004dcac-cae6-455e-a66e-f68518b0ea28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 6: DISTILLING KNOWLEDGE TO STUDENT MODEL\n",
            "================================================================================\n",
            "\n",
            "Why: Now we train the student to mimic the teacher's behavior while also\n",
            "learning from the true labels. This allows the small model to perform\n",
            "nearly as well as the large model.\n",
            "\n",
            "Training student model with knowledge distillation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Distilling Knowledge: 100%|██████████| 385/385 [00:47<00:00,  8.03it/s]\n",
            "Evaluating: 100%|██████████| 83/83 [00:00<00:00, 130.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "  Total Loss: 0.2883\n",
            "  Distillation Loss: 0.3601\n",
            "  Student CE Loss: 0.1207\n",
            "  Val Accuracy: 0.9538\n",
            "  ✓ New best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Distilling Knowledge: 100%|██████████| 385/385 [00:45<00:00,  8.47it/s]\n",
            "Evaluating: 100%|██████████| 83/83 [00:00<00:00, 128.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/5\n",
            "  Total Loss: 0.2344\n",
            "  Distillation Loss: 0.2925\n",
            "  Student CE Loss: 0.0990\n",
            "  Val Accuracy: 0.9568\n",
            "  ✓ New best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Distilling Knowledge: 100%|██████████| 385/385 [00:46<00:00,  8.31it/s]\n",
            "Evaluating: 100%|██████████| 83/83 [00:00<00:00, 133.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/5\n",
            "  Total Loss: 0.2159\n",
            "  Distillation Loss: 0.2703\n",
            "  Student CE Loss: 0.0892\n",
            "  Val Accuracy: 0.9629\n",
            "  ✓ New best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Distilling Knowledge: 100%|██████████| 385/385 [00:45<00:00,  8.39it/s]\n",
            "Evaluating: 100%|██████████| 83/83 [00:00<00:00, 127.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/5\n",
            "  Total Loss: 0.1819\n",
            "  Distillation Loss: 0.2267\n",
            "  Student CE Loss: 0.0773\n",
            "  Val Accuracy: 0.9606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Distilling Knowledge: 100%|██████████| 385/385 [00:46<00:00,  8.35it/s]\n",
            "Evaluating: 100%|██████████| 83/83 [00:00<00:00, 132.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/5\n",
            "  Total Loss: 0.1584\n",
            "  Distillation Loss: 0.1966\n",
            "  Student CE Loss: 0.0693\n",
            "  Val Accuracy: 0.9598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 7: FINAL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Evaluate teacher\n",
        "teacher_accuracy, teacher_preds, true_labels = evaluate_model(\n",
        "    teacher_model, teacher_test_loader, device\n",
        ")\n",
        "\n",
        "# Evaluate student\n",
        "student_accuracy, student_preds, _ = evaluate_model(\n",
        "    student_model, student_test_loader, device\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"KNOWLEDGE DISTILLATION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nTeacher Model Accuracy: {teacher_accuracy:.4f}\")\n",
        "print(f\"Student Model Accuracy: {student_accuracy:.4f}\")\n",
        "print(f\"Performance Retention: {(student_accuracy/teacher_accuracy)*100:.2f}%\")\n",
        "print(f\"\\nModel Size Reduction: {sum(p.numel() for p in teacher_model.parameters()) / sum(p.numel() for p in student_model.parameters()):.2f}x\")\n",
        "\n",
        "print(\"\\nTeacher Classification Report:\")\n",
        "print(classification_report(true_labels, teacher_preds,\n",
        "                          target_names=['Non-Imperative', 'Imperative']))\n",
        "\n",
        "print(\"\\nStudent Classification Report:\")\n",
        "print(classification_report(true_labels, student_preds,\n",
        "                          target_names=['Non-Imperative', 'Imperative']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k2qPCFKKa9q",
        "outputId": "f32d72b1-3fbe-4ecb-9f63-19700e1bbcb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 7: FINAL COMPARISON\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 83/83 [00:09<00:00,  8.87it/s]\n",
            "Evaluating: 100%|██████████| 83/83 [00:00<00:00, 104.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "KNOWLEDGE DISTILLATION RESULTS\n",
            "================================================================================\n",
            "\n",
            "Teacher Model Accuracy: 0.9765\n",
            "Student Model Accuracy: 0.9621\n",
            "Performance Retention: 98.53%\n",
            "\n",
            "Model Size Reduction: 24.96x\n",
            "\n",
            "Teacher Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Non-Imperative       0.99      0.97      0.98       658\n",
            "    Imperative       0.97      0.99      0.98       662\n",
            "\n",
            "      accuracy                           0.98      1320\n",
            "     macro avg       0.98      0.98      0.98      1320\n",
            "  weighted avg       0.98      0.98      0.98      1320\n",
            "\n",
            "\n",
            "Student Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Non-Imperative       0.96      0.96      0.96       658\n",
            "    Imperative       0.96      0.96      0.96       662\n",
            "\n",
            "      accuracy                           0.96      1320\n",
            "     macro avg       0.96      0.96      0.96      1320\n",
            "  weighted avg       0.96      0.96      0.96      1320\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 8: SAVING MODEL AND INFERENCE EXAMPLE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save the final model\n",
        "student_model.save_pretrained('./distilled_student_model')\n",
        "student_tokenizer.save_pretrained('./distilled_student_model')\n",
        "print(\"\\n✓ Student model saved to './distilled_student_model'\")\n",
        "\n",
        "# Inference example\n",
        "def predict_imperative(text, model, tokenizer, device):\n",
        "    \"\"\"Make prediction on new text.\"\"\"\n",
        "    model.eval()\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        probs = F.softmax(outputs.logits, dim=1)\n",
        "        pred = torch.argmax(probs, dim=1)\n",
        "\n",
        "    return pred.item(), probs[0].cpu().numpy()\n",
        "\n",
        "# Test examples\n",
        "test_sentences = [\n",
        "    \"Close the door.\",\n",
        "    \"The weather is nice today.\",\n",
        "    \"Please submit your assignment by Friday.\",\n",
        "    \"I went to the store yesterday.\"\n",
        "]\n",
        "\n",
        "print(\"\\nInference Examples:\")\n",
        "print(\"-\" * 60)\n",
        "for sentence in test_sentences:\n",
        "    pred, probs = predict_imperative(sentence, student_model, student_tokenizer, device)\n",
        "    label = \"Imperative\" if pred == 1 else \"Non-Imperative\"\n",
        "    confidence = probs[pred] * 100\n",
        "    print(f\"\\nSentence: '{sentence}'\")\n",
        "    print(f\"Prediction: {label} (confidence: {confidence:.2f}%)\")\n",
        "    print(f\"Probabilities: Non-Imp={probs[0]:.3f}, Imp={probs[1]:.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"KNOWLEDGE DISTILLATION COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nKey Takeaways:\")\n",
        "print(\"1. Student model is much smaller but retains most of teacher's performance\")\n",
        "print(\"2. Distillation transfers 'dark knowledge' - soft probabilities, not just labels\")\n",
        "print(\"3. Temperature parameter allows student to learn from teacher's uncertainty\")\n",
        "print(\"4. Result: Fast, efficient model suitable for production deployment!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3DYCGR_Kfby",
        "outputId": "1c86318a-5b47-4d9e-e45e-31e8c517acb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 8: SAVING MODEL AND INFERENCE EXAMPLE\n",
            "================================================================================\n",
            "\n",
            "✓ Student model saved to './distilled_student_model'\n",
            "\n",
            "Inference Examples:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Sentence: 'Close the door.'\n",
            "Prediction: Imperative (confidence: 99.98%)\n",
            "Probabilities: Non-Imp=0.000, Imp=1.000\n",
            "\n",
            "Sentence: 'The weather is nice today.'\n",
            "Prediction: Non-Imperative (confidence: 83.55%)\n",
            "Probabilities: Non-Imp=0.836, Imp=0.164\n",
            "\n",
            "Sentence: 'Please submit your assignment by Friday.'\n",
            "Prediction: Imperative (confidence: 99.98%)\n",
            "Probabilities: Non-Imp=0.000, Imp=1.000\n",
            "\n",
            "Sentence: 'I went to the store yesterday.'\n",
            "Prediction: Imperative (confidence: 99.97%)\n",
            "Probabilities: Non-Imp=0.000, Imp=1.000\n",
            "\n",
            "================================================================================\n",
            "KNOWLEDGE DISTILLATION COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "Key Takeaways:\n",
            "1. Student model is much smaller but retains most of teacher's performance\n",
            "2. Distillation transfers 'dark knowledge' - soft probabilities, not just labels\n",
            "3. Temperature parameter allows student to learn from teacher's uncertainty\n",
            "4. Result: Fast, efficient model suitable for production deployment!\n"
          ]
        }
      ]
    }
  ]
}